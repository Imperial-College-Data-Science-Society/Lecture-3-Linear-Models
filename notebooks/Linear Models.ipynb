{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Problem Definition](#problem-definition)\n",
    "    - [Linear Model](#linear-model)\n",
    "    - [Simplified CAPM](#simplified-capm)\n",
    "- [Target Function](#target-function)\n",
    "- [Raw Data](#raw-data)\n",
    "- [Preprocessing](#preprocessing)\n",
    "- [Machine Learning](#machine-learning)\n",
    "    - [Training](#training)\n",
    "    - [Model](#model)\n",
    "    - [Error Function](#error-function)\n",
    "- [Stochasic Gradient Descent](#stochastic-gradient-descent)\n",
    "    - [`NumPy`](#sgd-numpy)\n",
    "    - [`scikit-learn`](#sgd-sklearn)\n",
    "    - [`TensorFlow`](#sgd-tf)\n",
    "    - [`Keras`](#sgd-keras)\n",
    "- [Maximum Likelihood Estimator](#maximum-likelihood-estimator)\n",
    "    - [`NumPy`](#mle-numpy)\n",
    "    - [`scikit-learn`](#mle-sklearn)\n",
    "- [Challenge](#challenge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Definition <a class=\"anchor\" id=\"problem-definition\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provided a set $\\mathcal{S}$ of $(x_{i}, y_{i})$ pairs: \n",
    "    $$\\mathcal{S} = \\{ (x_{1}, y_{1}), (x_{2}, y_{2}), \\text{...}, (x_{k}, y_{k}) \\}$$\n",
    "Find a function $f$ that maps $x_{i} \\rightarrow y_{i}$:\n",
    "    $$f: \\mathcal{X} \\rightarrow \\mathcal{Y}$$\n",
    "\n",
    "$f$ is a:\n",
    "* **(Discriminate) Model**\n",
    "* **Map Function**\n",
    "* **Function Approximation**    \n",
    "for $\\mathcal{S}$, such that:\n",
    "    $$f(x_{i}) = \\hat{y_{i}} \\approx y_{i}, \\quad \\forall i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model <a class=\"anchor\" id=\"linear-model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f$ is a **Linear Model**, iff:\n",
    "\\begin{equation}\n",
    "    f(x_{i}) = w * x_{i}, \\quad x_{i} \\in \\mathbb{R}^{n}; y_{i} \\in \\mathbb{R}^{m}; w \\in \\mathbb{R}^{m \\times n}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of the $k^{th}$ datapoint for $n=2$ and $m=1$ is:\n",
    "$$ x_{k} = \\begin{bmatrix}\n",
    "               x_{k}^{(1)} & x_{k}^{(2)}\n",
    "           \\end{bmatrix}^{T}\n",
    "         = \\begin{bmatrix}\n",
    "               1 & 2\n",
    "           \\end{bmatrix}^{T}\n",
    "\\text{,} \\quad\n",
    "   y_{k} = \\begin{bmatrix}\n",
    "               y_{k}^{(1)}\n",
    "           \\end{bmatrix}\n",
    "         = \\begin{bmatrix}\n",
    "               3\n",
    "           \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a finite $\\mathcal{S}$ where $|\\mathcal{S}| = k$, we express the above equation in matrix format as:\n",
    "    $$f(\\mathbf{X}) = \\mathbf{\\hat{y}} = \\mathbf{X} * \\mathbf{w},\n",
    "    \\quad \\mathbf{X} \\in \\mathbb{R}^{k \\times n}; \\mathbf{y} \\in \\mathbb{R}^{k \\times m}; \\mathbf{w} \\in \\mathbb{R}^{n \\times m}$$\n",
    "    \n",
    "Matrices $\\mathbf{X}$ and $\\mathbf{y}$ are constructed by stacking the $k$ pairs as row vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplified CAPM (Capital Asset Pricing Model ) <a class=\"anchor\" id=\"simplified-capm\"></a>\n",
    "\n",
    "Factor models are a way of explaining the returns of one asset via a linear combination of the returns of other assets.\n",
    "The general form of a factor model is:\n",
    "\\begin{equation}\n",
    "    y = \\alpha + \\beta_{1} x_{1} + \\beta_{2} x_{2} + \\text{...} + \\beta_{n} x_{n}\n",
    "    \\label{eq::beta}\n",
    "\\end{equation}\n",
    "    \n",
    "An asset's $\\beta$ to another is just the $\\beta$ factor from the equation above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for:\n",
    "    $$y_{stock} = \\beta x_{benchmark} + \\alpha = \n",
    "        \\begin{bmatrix}\n",
    "           x_{benchmark} & 1\n",
    "       \\end{bmatrix}\n",
    "       \\begin{bmatrix}\n",
    "           \\beta \\\\\n",
    "           \\alpha\n",
    "       \\end{bmatrix}$$\n",
    "\n",
    "A sample of the dataset $S$ is visualized below:\n",
    "\n",
    "| $x_{benchmark}$  | $y_{stock}$ |\n",
    "| ---------------- | ----------- |\n",
    "| 5.0              | 1.6         |\n",
    "| 1.0              | 0.6         |\n",
    "| 3.7              | 1.3         |\n",
    "| 9.8              | 2.3         |\n",
    "| 2.8              | 0.8         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scientific computing library\n",
    "import numpy as np\n",
    "# machine learning library\n",
    "import sklearn\n",
    "\n",
    "# visualization tools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# show plots without need of calling `.show()`\n",
    "%matplotlib inline\n",
    "\n",
    "# prettify plots\n",
    "plt.rcParams['figure.figsize'] = [20.0, 5.0]\n",
    "sns.set_palette(sns.color_palette(\"muted\"))\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "# supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(x, y=None, y_noise=None, y_hat=None):\n",
    "    \"\"\"Visualization helper function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x: array-like\n",
    "        x-axis values\n",
    "    y: array-like\n",
    "        y-axis target values\n",
    "    y_noise: array-like\n",
    "        y-axis observations\n",
    "    y_hat: array-like\n",
    "        y-axis model predictions\n",
    "    \"\"\"\n",
    "    if y is not None:\n",
    "        plt.plot(x, y, label='Target Function')\n",
    "    if y_noise is not None:\n",
    "        plt.plot(x, y_noise, 'o', label='Noisy Observations')\n",
    "    if y_hat is not None:\n",
    "        plt.plot(x, y_hat, '--o', label='Model Predictions')\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Function <a class=\"anchor\" id=\"target-function\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "np.random.seed(0)\n",
    "\n",
    "x = np.linspace(0, N, N)\n",
    "y = 0.5 + 0.2 * x\n",
    "visualize(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Data <a class=\"anchor\" id=\"raw-data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_noise = y + np.random.normal(0,0.1, N)\n",
    "\n",
    "visualize(x, y, y_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing <a class=\"anchor\" id=\"preprocessing\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.ones((N, 2))\n",
    "X[:, 1 ] = x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning <a class=\"anchor\" id=\"machine-learning\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning is the science of getting computers to act without being explicitly programmed.\n",
    "Their course of action highly depends on the data that has been used in order to **train** them, while the generated program (a.k.a **model**) is a function of the data and the algorithm used to consume this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training <a class=\"anchor\" id=\"training\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is nothing more than the process of optimizating:\n",
    "$$\\mathcal{J}(w) = \\min_{w}\\bigg[ \\mathbb{E} \\big[ \\mathcal{L}(\\mathbf{y}, f(\\mathbf{X} | w)) \\big] + \\lambda \\mathcal{R}(w) \\bigg]$$\n",
    "\n",
    "where,\n",
    "* $\\mathcal{J}$: Objective Function\n",
    "* $\\mathcal{L}$: Loss Function\n",
    "* $\\mathcal{R}$: Regularization Term\n",
    "* $f$: Mapping Function (`model`)\n",
    "* $w$: `model` Parameters\n",
    "* $S = \\{ (x_{i}, y_{i}): i \\in \\mathcal{D} \\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumming that $\\mathcal{J}$ has a (global) minimum, such that:\n",
    "\n",
    "$\\DeclareMathOperator*{\\argmin}{arg\\,min}$\n",
    "$$w^{*} = \\argmin_{w}\\bigg[ \\mathbb{E} \\big[ \\mathcal{L}(\\mathbf{y}, f(\\mathbf{X} | w)) \\big] + \\lambda \\mathcal{R}(w) \\bigg]$$\n",
    "\n",
    "where,\n",
    "* $w^{*}$: Objective Function (Global) Minimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model <a class=\"anchor\" id=\"model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having found $w^{*}$ the `model` $f$ can be deployed and predictions can be made according to:\n",
    "\n",
    "$$\\mathbf{\\hat{y}} = f(\\mathbf{X} | w^{*}) \\approx \\mathbf{y}$$\n",
    "\n",
    "In the case of Linear Models, $f$ gets the form of a linear function, with $w^{*}$ coefficients:\n",
    "\n",
    "$$\\mathbf{\\hat{y}} = f(\\mathbf{X} | w^{*}) = \\mathbf{X} * w^{*} \\approx \\mathbf{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Function <a class=\"anchor\" id=\"error-function\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use **Mean Squared Error (MSE)** as the metric function in order to evaluate how good/bad the model $f$ is, such that:\n",
    "    $$MSE_{f} = \\frac{1}{k} \\sum_{i=1}^{k} (y_{i} - \\hat{y}_{i})^{2} = \\frac{1}{2k} \\sum_{i=1}^{k} (y_{i} - w_{i} * x_{i})^{2}$$\n",
    "In matrix form:\n",
    "    $$MSE_{f} = \\frac{1}{k} (\\mathbf{y} - \\mathbf{X} * \\mathbf{w})^{T}(\\mathbf{y} - \\mathbf{X} * \\mathbf{w})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can address the optimization problem of $MSE_{f}$:\n",
    "* computationally (Stochastic Gradient Descent)\n",
    "* analytically (Maximum Likelihood Estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent <a class=\"anchor\" id=\"stochastic-gradient-descent\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent (SGD) is the simplest and most used optimizer.\n",
    "It is based on making incremental updates, towards the correctsolution,\n",
    "using the derivative of the error at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$w_{t+1} = w_{t} - \\eta (y_{i} - \\hat{y}) x_{i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `NumPy` <a class=\"anchor\" id=\"sgd-numpy\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "n_epochs = 50\n",
    "eta = 0.01\n",
    "\n",
    "# parameters initialization\n",
    "w = np.random.random(X.shape[1])\n",
    "\n",
    "# training error over time\n",
    "loss = []\n",
    "\n",
    "# model fitting\n",
    "for _ in range(n_epochs):\n",
    "    y_hat = np.dot(X, w)\n",
    "    l = 0.5 * np.sum((y_noise - y_hat)**2)\n",
    "    i = np.random.randint(len(y_noise))\n",
    "    error = y_hat[i] - y_noise[i]\n",
    "    w = w - eta * error * X[i]\n",
    "    loss.append(l)\n",
    "\n",
    "# plot results\n",
    "visualize(x, y, y_noise, y_hat)\n",
    "\n",
    "# plot training error\n",
    "plt.figure()\n",
    "plt.plot(loss)\n",
    "plt.title('Learning Curve')\n",
    "plt.legend(['Training Error']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `scikit-learn`  <a class=\"anchor\" id=\"sgd-sklearn\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# hyperparameters\n",
    "n_epochs = 100\n",
    "eta = 0.01\n",
    "\n",
    "# model initialization\n",
    "model = SGDRegressor(loss='squared_loss', penalty='none', max_iter=n_epochs, learning_rate='constant', eta0=eta)\n",
    "\n",
    "# model fitting & predictions\n",
    "model.fit(X, y_noise)\n",
    "y_hat = model.predict(X).ravel()\n",
    "\n",
    "# plot results\n",
    "visualize(x, y, y_noise, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimator  <a class=\"anchor\" id=\"maximum-likelihood-estimator\"></a>\n",
    "\n",
    "Differentiate $MSE_{f}(w)$ and find its minimum. There is a global minimiser, since $MSE_{f}$ is convex (quadratic).\n",
    "\n",
    "$$\\mathbf{w_{MLE}} = (\\mathbf{X}^{T} \\mathbf{X})^{-1} * \\mathbf{X}^{T} * \\mathbf{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `NumPy` <a class=\"anchor\" id=\"mle-numpy\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), y_noise)\n",
    "y_hat = np.dot(X, w)\n",
    "\n",
    "visualize(x, y, y_noise, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `scikit-learn` <a class=\"anchor\" id=\"mle-sklearn\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# model initialization\n",
    "model = LinearRegression()\n",
    "\n",
    "# model fitting & predictions\n",
    "model.fit(X, y_noise)\n",
    "y_hat = model.predict(X).ravel()\n",
    "\n",
    "# plot results\n",
    "visualize(x, y, y_noise, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges <a class=\"anchor\" id=\"challenge\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beta Hedging <a class=\"anchor\" id=\"beta-hedging\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the above experiment using the implementation you prefer most, on real world market data.<br>\n",
    "Using the data provided in `{proj_dir}/notebooks/data/`:\n",
    "* **S&P500 (SPY)**: as the market index (benchmark)\n",
    "* **Apple (AAPL)**: as stock A\n",
    "* **Google (GOOG)** as stocks B<br>\n",
    "\n",
    "come up with a <u>Beta Hedging Strategy</u> to minimize risk.<br><br>\n",
    "\n",
    "Load the data to a numpy.ndarray using the commands below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper loader function\n",
    "loader = lambda asset: np.loadtxt('./data/%s.txt' % asset, delimiter=',')\n",
    "\n",
    "# raw data\n",
    "spy = loader('SPY')\n",
    "aapl = loader('AAPL')\n",
    "goog = loader('GOOG')\n",
    "\n",
    "# normalised data\n",
    "spy_ = spy / spy[0] - 1\n",
    "aapl_ = aapl / aapl[0] - 1\n",
    "goog_ = goog / goog[0] - 1\n",
    "\n",
    "# visualization\n",
    "plt.plot(spy_, label='S&P500')\n",
    "plt.plot(aapl_, label='Apple Inc.')\n",
    "plt.plot(goog_, label='Alphabet Inc.')\n",
    "plt.title('Normalised Prices')\n",
    "plt.ylabel('Normalised Change')\n",
    "plt.xlabel('Time Index')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression plots\n",
    "sns.regplot(spy_, aapl_, label='APPL')\n",
    "sns.regplot(spy_, goog_, label='GOOG')\n",
    "plt.title('Beta Exposure to S&P500')\n",
    "plt.xlabel('SPY Percentage Change')\n",
    "plt.ylabel('Stock Price Percentage Change')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper percentage return\n",
    "pct_change = lambda series: np.diff(series) / (series[:-1] + 1e-6) # WARNING: prone to division by zero\n",
    "\n",
    "aapl_pct = pct_change(aapl)\n",
    "goog_pct = pct_change(goog)\n",
    "spy_pct = pct_change(spy)\n",
    "\n",
    "sns.distplot(aapl_pct, label='AAPL')\n",
    "sns.distplot(goog_pct, label='GOOG')\n",
    "sns.distplot(spy_pct, label='SPY')\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
